{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.utils import shuffle\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Import Keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "from time import time\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = os.path.dirname(\"\")\n",
    "\n",
    "# read csv file\n",
    "df = pd.read_csv(os.path.join(path_dir, 'train_ts.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_dir = os.path.dirname(\"\")\n",
    "\n",
    "# read csv file\n",
    "df = pd.read_csv(os.path.join(path_dir, 'train_ts.csv'))\n",
    "\n",
    "# shuffle data\n",
    "df = shuffle(df)\n",
    "\n",
    "df['timeseries'] = df['timeseries'].apply(lambda x : x.replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\"))\n",
    "df['pheno_metrics'] = df['pheno_metrics'].apply(lambda x : x.replace(\"[\",\"\").replace(\"]\",\"\").replace(\" \",\"\"))\n",
    "\n",
    "df['timeseries_pheno_metrics'] = df['timeseries'] +','+df['pheno_metrics']\n",
    "df['timeseries_pheno_metrics'] = df['timeseries_pheno_metrics'].apply(lambda x : x.split(','))\n",
    "df['timeseries_pheno_metrics'] = df['timeseries_pheno_metrics'].apply(lambda x : [float(i) for i in x])\n",
    "df['timeseries_pheno_metrics'] = df['timeseries_pheno_metrics'].apply(lambda x : np.array(x, dtype=np.float32) )\n",
    "\n",
    "X_train = df['timeseries_pheno_metrics'] \n",
    "y_train = df[\"label_id\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((862,), (862,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "(733,) (733,) (129,) (129,)\n"
     ]
    }
   ],
   "source": [
    "T = 23\n",
    "\n",
    "samples = 129\n",
    "\n",
    "train_timeseries = X_train[samples:]\n",
    "train_labels = y_train[samples:]\n",
    "\n",
    "test_timeseries = X_train[:samples]\n",
    "test_labels = y_train[:samples]\n",
    "\n",
    "num_classes = len(np.unique(y_train))\n",
    "\n",
    "print(num_classes)\n",
    "\n",
    "print(train_timeseries.shape, train_labels.shape, test_timeseries.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train data dimensions: (711, 23, 1), (711, 1)\n",
      "Test data dimensions: (129, 23, 1), (129, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = [], []\n",
    "for i in range(train_labels.shape[0] - (T-1)):\n",
    "    X_train.append(np.expand_dims(train_timeseries.iloc[i:i+T].values, axis=1))\n",
    "    y_train.append(train_labels.iloc[i + (T-1)])\n",
    "X_train, y_train = np.array(X_train), np.array(y_train).reshape(-1,1)\n",
    "print(f'Train data dimensions: {X_train.shape}, {y_train.shape}')\n",
    "\n",
    "X_test, y_test = [], []\n",
    "for i in range(test_labels.shape[0]):\n",
    "    X_test.append(np.expand_dims(train_timeseries.iloc[i:i+T].values, axis=1))\n",
    "    y_test.append(test_labels.iloc[i])\n",
    "X_test, y_test = np.array(X_test), np.array(y_test).reshape(-1,1)  \n",
    "\n",
    "print(f'Test data dimensions: {X_test.shape}, {y_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.0330000e+03,  8.9820000e+03,  8.9770000e+03,  9.0570000e+03,\n",
       "        9.0360000e+03,  9.0400000e+03,  8.9040000e+03,  9.0230000e+03,\n",
       "        9.0020000e+03,  8.9170000e+03,  8.8590000e+03,  8.9370000e+03,\n",
       "        8.9270000e+03,  8.8930000e+03,  8.7330000e+03,  8.2850000e+03,\n",
       "        8.7900000e+03,  8.8860000e+03, -9.9990000e+03, -9.9990000e+03,\n",
       "        9.0650000e+03,  8.9520000e+03,  9.3430000e+03,  9.1391426e+03,\n",
       "        4.0295834e+02,            nan,  0.0000000e+00,  0.0000000e+00,\n",
       "        1.5845500e+05,  3.7843300e+05,  1.9342000e+04, -3.2800000e+02,\n",
       "        4.8000000e+01, -9.9990000e+03,  3.0500000e+02,  9.3430000e+03,\n",
       "        3.5300000e+02, -9.9990000e+03,  2.8900000e+02,            nan,\n",
       "        0.0000000e+00], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYERS = [8, 8, 8, 1]                # number of units in hidden and output layers\n",
    "M_TRAIN = X_train.shape[0]           # number of training examples (2D)\n",
    "M_TEST = X_test.shape[0]             # number of test examples (2D),full=X_test.shape[0]\n",
    "N = X_train.shape[2]                 # number of features\n",
    "BATCH = M_TRAIN                          # batch size\n",
    "EPOCH = 50                           # number of epochs\n",
    "LR = 5e-2                            # learning rate of the gradient descent\n",
    "LAMBD = 3e-2                         # lambda in L2 regularizaion\n",
    "DP = 0.0                             # dropout rate\n",
    "RDP = 0.0                            # recurrent dropout rate\n",
    "print(f'layers={LAYERS}, train_examples={M_TRAIN}, test_examples={M_TEST}')\n",
    "print(f'batch = {BATCH}, timesteps = {T}, features = {N}, epochs = {EPOCH}')\n",
    "print(f'lr = {LR}, lambda = {LAMBD}, dropout = {DP}, recurr_dropout = {RDP}')\n",
    "\n",
    "# Build the Model\n",
    "model = Sequential()\n",
    "model.add(LSTM(input_shape=(T, N), units=LAYERS[0],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "               dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=LAYERS[1],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "               dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=True, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(LSTM(units=LAYERS[2],\n",
    "               activation='tanh', recurrent_activation='hard_sigmoid',\n",
    "               kernel_regularizer=l2(LAMBD), recurrent_regularizer=l2(LAMBD),\n",
    "               dropout=DP, recurrent_dropout=RDP,\n",
    "               return_sequences=False, return_state=False,\n",
    "               stateful=False, unroll=False\n",
    "              ))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=LAYERS[3], activation='sigmoid'))\n",
    "\n",
    "# Compile the model with Adam optimizer\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              optimizer=Adam(learning_rate=LR))\n",
    "print(model.summary())\n",
    "\n",
    "# Define a learning rate decay method:\n",
    "lr_decay = ReduceLROnPlateau(monitor='loss', \n",
    "                             patience=1, verbose=0, \n",
    "                             factor=0.5, min_learning_rate=1e-8)\n",
    "# Define Early Stopping:\n",
    "early_stop = EarlyStopping(monitor='val_acc', min_delta=0, \n",
    "                           patience=30, verbose=1, mode='auto',\n",
    "                           baseline=0, restore_best_weights=True)\n",
    "# Train the model. \n",
    "# The dataset is small for NN - let's use test_data for validation\n",
    "start = time()\n",
    "History = model.fit(X_train, y_train,\n",
    "                    epochs=EPOCH,\n",
    "                    batch_size=BATCH,\n",
    "                    validation_split=0.0,\n",
    "                    validation_data=(X_test[:M_TEST], y_test[:M_TEST]),\n",
    "                    shuffle=True,verbose=0,\n",
    "                    callbacks=[lr_decay, early_stop])\n",
    "print('-'*65)\n",
    "print(f'Training was completed in {time() - start:.2f} secs')\n",
    "print('-'*65)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
